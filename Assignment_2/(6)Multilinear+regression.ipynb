{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tpot in c:\\users\\hp\\anaconda32\\lib\\site-packages\n",
      "Requirement already satisfied: stopit>=1.1.1 in c:\\users\\hp\\anaconda32\\lib\\site-packages (from tpot)\n",
      "Requirement already satisfied: pandas>=0.20.2 in c:\\users\\hp\\anaconda32\\lib\\site-packages (from tpot)\n",
      "Requirement already satisfied: tqdm>=4.11.2 in c:\\users\\hp\\anaconda32\\lib\\site-packages (from tpot)\n",
      "Requirement already satisfied: update-checker>=0.16 in c:\\users\\hp\\anaconda32\\lib\\site-packages (from tpot)\n",
      "Requirement already satisfied: numpy>=1.12.1 in c:\\users\\hp\\anaconda32\\lib\\site-packages (from tpot)\n",
      "Requirement already satisfied: deap>=1.0 in c:\\users\\hp\\anaconda32\\lib\\site-packages (from tpot)\n",
      "Requirement already satisfied: scikit-learn>=0.18.1 in c:\\users\\hp\\anaconda32\\lib\\site-packages (from tpot)\n",
      "Requirement already satisfied: scipy>=0.19.0 in c:\\users\\hp\\anaconda32\\lib\\site-packages (from tpot)\n",
      "Requirement already satisfied: python-dateutil>=2 in c:\\users\\hp\\anaconda32\\lib\\site-packages (from pandas>=0.20.2->tpot)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\hp\\anaconda32\\lib\\site-packages (from pandas>=0.20.2->tpot)\n",
      "Requirement already satisfied: requests>=2.3.0 in c:\\users\\hp\\anaconda32\\lib\\site-packages (from update-checker>=0.16->tpot)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda32\\lib\\site-packages (from python-dateutil>=2->pandas>=0.20.2->tpot)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\hp\\anaconda32\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\hp\\anaconda32\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\hp\\anaconda32\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda32\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot)\n"
     ]
    }
   ],
   "source": [
    "!pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## imporing the librarires\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.pylab import rcParams\n",
    "import seaborn as sns \n",
    "from tpot import TPOTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Energy_consumed</th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>Kitchen_Temp</th>\n",
       "      <th>Kitchen_Hum</th>\n",
       "      <th>LivingRoom_Temp</th>\n",
       "      <th>LivingRoom_Hum</th>\n",
       "      <th>...</th>\n",
       "      <th>ParentRoom_Hum</th>\n",
       "      <th>Outside_Temp</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>rv1</th>\n",
       "      <th>Month_Number</th>\n",
       "      <th>Weekday_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>17:00:00</td>\n",
       "      <td>90</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.89</td>\n",
       "      <td>47.596667</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>...</td>\n",
       "      <td>45.53</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>733.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>13.275433</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>17:10:00</td>\n",
       "      <td>90</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.89</td>\n",
       "      <td>46.693333</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.722500</td>\n",
       "      <td>...</td>\n",
       "      <td>45.56</td>\n",
       "      <td>6.483333</td>\n",
       "      <td>733.6</td>\n",
       "      <td>92.0</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>59.166667</td>\n",
       "      <td>5.2</td>\n",
       "      <td>18.606195</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>17:20:00</td>\n",
       "      <td>80</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>19.89</td>\n",
       "      <td>46.300000</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.626667</td>\n",
       "      <td>...</td>\n",
       "      <td>45.50</td>\n",
       "      <td>6.366667</td>\n",
       "      <td>733.7</td>\n",
       "      <td>92.0</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>55.333333</td>\n",
       "      <td>5.1</td>\n",
       "      <td>28.642668</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>17:30:00</td>\n",
       "      <td>90</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>19.89</td>\n",
       "      <td>46.066667</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.590000</td>\n",
       "      <td>...</td>\n",
       "      <td>45.40</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>733.8</td>\n",
       "      <td>92.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>45.410389</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>17:40:00</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>19.89</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.530000</td>\n",
       "      <td>...</td>\n",
       "      <td>45.40</td>\n",
       "      <td>6.133333</td>\n",
       "      <td>733.9</td>\n",
       "      <td>92.0</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>47.666667</td>\n",
       "      <td>4.9</td>\n",
       "      <td>10.084097</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        date      Time  Energy_consumed  Appliances  lights  \\\n",
       "0           0  2016-01-11  17:00:00               90          60      30   \n",
       "1           1  2016-01-11  17:10:00               90          60      30   \n",
       "2           2  2016-01-11  17:20:00               80          50      30   \n",
       "3           3  2016-01-11  17:30:00               90          50      40   \n",
       "4           4  2016-01-11  17:40:00              100          60      40   \n",
       "\n",
       "   Kitchen_Temp  Kitchen_Hum  LivingRoom_Temp  LivingRoom_Hum       ...        \\\n",
       "0         19.89    47.596667             19.2       44.790000       ...         \n",
       "1         19.89    46.693333             19.2       44.722500       ...         \n",
       "2         19.89    46.300000             19.2       44.626667       ...         \n",
       "3         19.89    46.066667             19.2       44.590000       ...         \n",
       "4         19.89    46.333333             19.2       44.530000       ...         \n",
       "\n",
       "   ParentRoom_Hum  Outside_Temp  Pressure  Humidity  Windspeed  Visibility  \\\n",
       "0           45.53      6.600000     733.5      92.0   7.000000   63.000000   \n",
       "1           45.56      6.483333     733.6      92.0   6.666667   59.166667   \n",
       "2           45.50      6.366667     733.7      92.0   6.333333   55.333333   \n",
       "3           45.40      6.250000     733.8      92.0   6.000000   51.500000   \n",
       "4           45.40      6.133333     733.9      92.0   5.666667   47.666667   \n",
       "\n",
       "   Tdewpoint        rv1  Month_Number  Weekday_number  \n",
       "0        5.3  13.275433             1               0  \n",
       "1        5.2  18.606195             1               0  \n",
       "2        5.1  28.642668             1               0  \n",
       "3        5.0  45.410389             1               0  \n",
       "4        4.9  10.084097             1               0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing the dataset\n",
    "dataset = pd.read_csv(\"G://ADS//Assignment 2//Modified_energydata_complete(2).csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "## X are the varibles used for predictions\n",
    "## Y is the target\n",
    "X = dataset.iloc[:,6:33].values\n",
    "Y = dataset.iloc[:,3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## fitting multiple linear regression to the training set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=np.random)\n",
    "regressor.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  53.79530162,  107.21766618,   85.76481756, ...,   77.46966837,\n",
       "        109.26695819,  107.25250077])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predictions = regressor.predict(X_test)\n",
    "#print(regressor.intercept_)\n",
    "#print(regressor.coef_)\n",
    "Y_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f623934470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Y_test,Y_predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f623875fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Residual Distribution Plot\n",
    "sns.distplot((Y_test-Y_predictions))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th>  <td>   0.157</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.156</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   135.6</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 16 Mar 2018</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:55:49</td>     <th>  Log-Likelihood:    </th> <td>-1.1805e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 19735</td>      <th>  AIC:               </th>  <td>2.362e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 19707</td>      <th>  BIC:               </th>  <td>2.364e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    27</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   23.3487</td> <td>   96.786</td> <td>    0.241</td> <td> 0.809</td> <td> -166.360</td> <td>  213.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.6513</td> <td>    1.896</td> <td>   -0.344</td> <td> 0.731</td> <td>   -4.367</td> <td>    3.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   16.5571</td> <td>    0.684</td> <td>   24.204</td> <td> 0.000</td> <td>   15.216</td> <td>   17.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>  -21.7007</td> <td>    1.667</td> <td>  -13.020</td> <td> 0.000</td> <td>  -24.968</td> <td>  -18.434</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>  -14.7222</td> <td>    0.784</td> <td>  -18.778</td> <td> 0.000</td> <td>  -16.259</td> <td>  -13.185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   26.4460</td> <td>    1.086</td> <td>   24.343</td> <td> 0.000</td> <td>   24.317</td> <td>   28.575</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    4.5670</td> <td>    0.696</td> <td>    6.563</td> <td> 0.000</td> <td>    3.203</td> <td>    5.931</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    6.8785</td> <td>    1.020</td> <td>    6.742</td> <td> 0.000</td> <td>    4.879</td> <td>    8.878</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    2.4620</td> <td>    0.652</td> <td>    3.775</td> <td> 0.000</td> <td>    1.184</td> <td>    3.740</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.8243</td> <td>    1.205</td> <td>    0.684</td> <td> 0.494</td> <td>   -1.538</td> <td>    3.187</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>    0.2600</td> <td>    0.089</td> <td>    2.927</td> <td> 0.003</td> <td>    0.086</td> <td>    0.434</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>    7.3409</td> <td>    0.654</td> <td>   11.233</td> <td> 0.000</td> <td>    6.060</td> <td>    8.622</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>    0.1460</td> <td>    0.077</td> <td>    1.906</td> <td> 0.057</td> <td>   -0.004</td> <td>    0.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>   -0.3661</td> <td>    1.367</td> <td>   -0.268</td> <td> 0.789</td> <td>   -3.045</td> <td>    2.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>   -2.0700</td> <td>    0.439</td> <td>   -4.711</td> <td> 0.000</td> <td>   -2.931</td> <td>   -1.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>   11.0746</td> <td>    0.990</td> <td>   11.182</td> <td> 0.000</td> <td>    9.133</td> <td>   13.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>   -6.4024</td> <td>    0.378</td> <td>  -16.959</td> <td> 0.000</td> <td>   -7.142</td> <td>   -5.662</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>  -21.1951</td> <td>    1.872</td> <td>  -11.319</td> <td> 0.000</td> <td>  -24.865</td> <td>  -17.525</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>   -2.0727</td> <td>    0.419</td> <td>   -4.943</td> <td> 0.000</td> <td>   -2.895</td> <td>   -1.251</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>   -8.3297</td> <td>    1.603</td> <td>   -5.197</td> <td> 0.000</td> <td>  -11.471</td> <td>   -5.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>    0.0785</td> <td>    0.110</td> <td>    0.717</td> <td> 0.474</td> <td>   -0.136</td> <td>    0.293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>   -0.2619</td> <td>    0.333</td> <td>   -0.788</td> <td> 0.431</td> <td>   -0.914</td> <td>    0.390</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>    1.9925</td> <td>    0.357</td> <td>    5.588</td> <td> 0.000</td> <td>    1.294</td> <td>    2.691</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>    0.1848</td> <td>    0.059</td> <td>    3.119</td> <td> 0.002</td> <td>    0.069</td> <td>    0.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>    3.9531</td> <td>    1.520</td> <td>    2.601</td> <td> 0.009</td> <td>    0.974</td> <td>    6.932</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>   -0.0436</td> <td>    0.047</td> <td>   -0.925</td> <td> 0.355</td> <td>   -0.136</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>  -12.5759</td> <td>    1.781</td> <td>   -7.060</td> <td> 0.000</td> <td>  -16.067</td> <td>   -9.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>    1.0901</td> <td>    0.361</td> <td>    3.017</td> <td> 0.003</td> <td>    0.382</td> <td>    1.798</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13799.743</td> <th>  Durbin-Watson:     </th>  <td>   0.601</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>202481.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 3.278</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>17.257</td>   <th>  Cond. No.          </th>  <td>1.10e+05</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.157\n",
       "Model:                            OLS   Adj. R-squared:                  0.156\n",
       "Method:                 Least Squares   F-statistic:                     135.6\n",
       "Date:                Fri, 16 Mar 2018   Prob (F-statistic):               0.00\n",
       "Time:                        17:55:49   Log-Likelihood:            -1.1805e+05\n",
       "No. Observations:               19735   AIC:                         2.362e+05\n",
       "Df Residuals:                   19707   BIC:                         2.364e+05\n",
       "Df Model:                          27                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         23.3487     96.786      0.241      0.809    -166.360     213.058\n",
       "x1            -0.6513      1.896     -0.344      0.731      -4.367       3.064\n",
       "x2            16.5571      0.684     24.204      0.000      15.216      17.898\n",
       "x3           -21.7007      1.667    -13.020      0.000     -24.968     -18.434\n",
       "x4           -14.7222      0.784    -18.778      0.000     -16.259     -13.185\n",
       "x5            26.4460      1.086     24.343      0.000      24.317      28.575\n",
       "x6             4.5670      0.696      6.563      0.000       3.203       5.931\n",
       "x7             6.8785      1.020      6.742      0.000       4.879       8.878\n",
       "x8             2.4620      0.652      3.775      0.000       1.184       3.740\n",
       "x9             0.8243      1.205      0.684      0.494      -1.538       3.187\n",
       "x10            0.2600      0.089      2.927      0.003       0.086       0.434\n",
       "x11            7.3409      0.654     11.233      0.000       6.060       8.622\n",
       "x12            0.1460      0.077      1.906      0.057      -0.004       0.296\n",
       "x13           -0.3661      1.367     -0.268      0.789      -3.045       2.313\n",
       "x14           -2.0700      0.439     -4.711      0.000      -2.931      -1.209\n",
       "x15           11.0746      0.990     11.182      0.000       9.133      13.016\n",
       "x16           -6.4024      0.378    -16.959      0.000      -7.142      -5.662\n",
       "x17          -21.1951      1.872    -11.319      0.000     -24.865     -17.525\n",
       "x18           -2.0727      0.419     -4.943      0.000      -2.895      -1.251\n",
       "x19           -8.3297      1.603     -5.197      0.000     -11.471      -5.188\n",
       "x20            0.0785      0.110      0.717      0.474      -0.136       0.293\n",
       "x21           -0.2619      0.333     -0.788      0.431      -0.914       0.390\n",
       "x22            1.9925      0.357      5.588      0.000       1.294       2.691\n",
       "x23            0.1848      0.059      3.119      0.002       0.069       0.301\n",
       "x24            3.9531      1.520      2.601      0.009       0.974       6.932\n",
       "x25           -0.0436      0.047     -0.925      0.355      -0.136       0.049\n",
       "x26          -12.5759      1.781     -7.060      0.000     -16.067      -9.084\n",
       "x27            1.0901      0.361      3.017      0.003       0.382       1.798\n",
       "==============================================================================\n",
       "Omnibus:                    13799.743   Durbin-Watson:                   0.601\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           202481.634\n",
       "Skew:                           3.278   Prob(JB):                         0.00\n",
       "Kurtosis:                      17.257   Cond. No.                     1.10e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.1e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "X = np.append(arr=np.ones((19735,1)).astype(int),values = X,axis = 1)\n",
    "X_opt = X[:,0:32]\n",
    "regressor_OLS = sm.OLS(endog = Y,exog = X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15051068206631391"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16033855615153714"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.score(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-square,RMSE,MAE,MAPE (TRAINING DATA)Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting the test set results\n",
    "train_pred =regressor.predict(X_train)\n",
    "\n",
    "##  MAPE\n",
    "import numpy as np\n",
    "def mean_absolute_percentage_error(Y_train,train_pred): \n",
    "    Y_train, train_pred = np.array(Y_train), np.array(train_pred)\n",
    "    return np.mean(np.abs((Y_train,train_pred) / Y_train)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Training Data = 54.8791980833\n",
      "RMSE of Training Data = 95.81636471985377\n",
      "R2 of Training Data = 0.160338556152\n",
      "MAPE of Training Data = 121.126774449\n"
     ]
    }
   ],
   "source": [
    "## R-squared score of this model\n",
    "from sklearn.metrics import*\n",
    "import math\n",
    "## Mean absolute error\n",
    "MAE = mean_absolute_error(Y_train,train_pred)\n",
    "print('MAE of Training Data =', MAE)\n",
    "## Mean squared error\n",
    "MSE = mean_squared_error(Y_train,train_pred)\n",
    "RMSE = math.sqrt(MSE)\n",
    "print('RMSE of Training Data =',RMSE)\n",
    "## R-square score of this model\n",
    "R2 = r2_score(Y_train,train_pred)\n",
    "print('R2 of Training Data =',R2)\n",
    "## MAPE of this model\n",
    "MAPE=mean_absolute_percentage_error(Y_train,train_pred)\n",
    "print('MAPE of Training Data =',MAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-square,RMSE,MAE,MAPE (TESTING DATA)Â¶Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predicting the test set results\n",
    "test_pred =regressor.predict(X_test)\n",
    "\n",
    "## MAPE\n",
    "import numpy as np\n",
    "def mean_absolute_percentage_error(Y_test,test_pred): \n",
    "    Y_test, test_pred = np.array(Y_test), np.array(test_pred)\n",
    "    return np.mean(np.abs((Y_test,test_pred) / Y_test)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Testing Data= 55.1613328747\n",
      "RMSE of Testing Data= 96.0730899330311\n",
      "R2 of Testing Data= 0.140401776372\n",
      "MAPE of Testing Data= 122.353307696\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "##Mean absolute error\n",
    "MAE = mean_absolute_error(Y_test,test_pred)\n",
    "print('MAE of Testing Data=',MAE)\n",
    "## Mean squared error\n",
    "MSE = mean_squared_error(Y_test,test_pred)\n",
    "RMSE = math.sqrt(MSE)\n",
    "print('RMSE of Testing Data=',RMSE)\n",
    "## R-square score of this model\n",
    "R2 = r2_score(Y_test,test_pred)\n",
    "print('R2 of Testing Data=',R2)\n",
    "## MAPE of this model\n",
    "MAPE=mean_absolute_percentage_error(Y_test,test_pred)\n",
    "print('MAPE of Testing Data=',MAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE SELECTION AFTER BORUTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2)\n",
    "#tpot.fit(X_train, Y_train)\n",
    "#print(tpot.score(X_test, Y_test))\n",
    "#tpot.export('tpot_iris_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## X are the varibles used for predictions\n",
    "## Y is the target\n",
    "X = dataset.iloc[:,[7,8,9,11,13,15,16,17,19,20,21,23,24,25,26,29]].values\n",
    "Y = dataset.iloc[:,3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## fitting multiple linear regression to the training set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=np.random)\n",
    "regressor.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th>  <td>   0.115</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.114</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   159.9</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 16 Mar 2018</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:33:46</td>     <th>  Log-Likelihood:    </th> <td>-1.1853e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 19735</td>      <th>  AIC:               </th>  <td>2.371e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 19718</td>      <th>  BIC:               </th>  <td>2.372e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    16</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>  176.5734</td> <td>   90.297</td> <td>    1.955</td> <td> 0.051</td> <td>   -0.417</td> <td>  353.564</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   14.4408</td> <td>    0.607</td> <td>   23.779</td> <td> 0.000</td> <td>   13.251</td> <td>   15.631</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>  -11.8819</td> <td>    0.988</td> <td>  -12.024</td> <td> 0.000</td> <td>  -13.819</td> <td>   -9.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>  -11.9967</td> <td>    0.627</td> <td>  -19.142</td> <td> 0.000</td> <td>  -13.225</td> <td>  -10.768</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    7.2320</td> <td>    0.692</td> <td>   10.452</td> <td> 0.000</td> <td>    5.876</td> <td>    8.588</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    2.4861</td> <td>    0.573</td> <td>    4.340</td> <td> 0.000</td> <td>    1.363</td> <td>    3.609</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.2314</td> <td>    0.087</td> <td>    2.654</td> <td> 0.008</td> <td>    0.061</td> <td>    0.402</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    5.4992</td> <td>    0.619</td> <td>    8.878</td> <td> 0.000</td> <td>    4.285</td> <td>    6.713</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    0.3044</td> <td>    0.065</td> <td>    4.681</td> <td> 0.000</td> <td>    0.177</td> <td>    0.432</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>   -2.7674</td> <td>    0.403</td> <td>   -6.862</td> <td> 0.000</td> <td>   -3.558</td> <td>   -1.977</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>    9.1791</td> <td>    0.756</td> <td>   12.136</td> <td> 0.000</td> <td>    7.697</td> <td>   10.662</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>   -5.2951</td> <td>    0.356</td> <td>  -14.874</td> <td> 0.000</td> <td>   -5.993</td> <td>   -4.597</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>   -2.1600</td> <td>    0.399</td> <td>   -5.414</td> <td> 0.000</td> <td>   -2.942</td> <td>   -1.378</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>   -9.4982</td> <td>    1.527</td> <td>   -6.220</td> <td> 0.000</td> <td>  -12.491</td> <td>   -6.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>   -0.0339</td> <td>    0.105</td> <td>   -0.324</td> <td> 0.746</td> <td>   -0.239</td> <td>    0.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>   -1.2144</td> <td>    0.311</td> <td>   -3.900</td> <td> 0.000</td> <td>   -1.825</td> <td>   -0.604</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>    6.1321</td> <td>    1.489</td> <td>    4.119</td> <td> 0.000</td> <td>    3.214</td> <td>    9.050</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13730.883</td> <th>  Durbin-Watson:     </th>  <td>   0.566</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>192661.331</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 3.274</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>16.835</td>   <th>  Cond. No.          </th>  <td>9.96e+04</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.115\n",
       "Model:                            OLS   Adj. R-squared:                  0.114\n",
       "Method:                 Least Squares   F-statistic:                     159.9\n",
       "Date:                Fri, 16 Mar 2018   Prob (F-statistic):               0.00\n",
       "Time:                        15:33:46   Log-Likelihood:            -1.1853e+05\n",
       "No. Observations:               19735   AIC:                         2.371e+05\n",
       "Df Residuals:                   19718   BIC:                         2.372e+05\n",
       "Df Model:                          16                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        176.5734     90.297      1.955      0.051      -0.417     353.564\n",
       "x1            14.4408      0.607     23.779      0.000      13.251      15.631\n",
       "x2           -11.8819      0.988    -12.024      0.000     -13.819      -9.945\n",
       "x3           -11.9967      0.627    -19.142      0.000     -13.225     -10.768\n",
       "x4             7.2320      0.692     10.452      0.000       5.876       8.588\n",
       "x5             2.4861      0.573      4.340      0.000       1.363       3.609\n",
       "x6             0.2314      0.087      2.654      0.008       0.061       0.402\n",
       "x7             5.4992      0.619      8.878      0.000       4.285       6.713\n",
       "x8             0.3044      0.065      4.681      0.000       0.177       0.432\n",
       "x9            -2.7674      0.403     -6.862      0.000      -3.558      -1.977\n",
       "x10            9.1791      0.756     12.136      0.000       7.697      10.662\n",
       "x11           -5.2951      0.356    -14.874      0.000      -5.993      -4.597\n",
       "x12           -2.1600      0.399     -5.414      0.000      -2.942      -1.378\n",
       "x13           -9.4982      1.527     -6.220      0.000     -12.491      -6.505\n",
       "x14           -0.0339      0.105     -0.324      0.746      -0.239       0.171\n",
       "x15           -1.2144      0.311     -3.900      0.000      -1.825      -0.604\n",
       "x16            6.1321      1.489      4.119      0.000       3.214       9.050\n",
       "==============================================================================\n",
       "Omnibus:                    13730.883   Durbin-Watson:                   0.566\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           192661.331\n",
       "Skew:                           3.274   Prob(JB):                         0.00\n",
       "Kurtosis:                      16.835   Cond. No.                     9.96e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 9.96e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "X = np.append(arr=np.ones((19735,1)).astype(int),values = X,axis = 1)\n",
    "X_opt = X[:,0:32]\n",
    "regressor_OLS = sm.OLS(endog = Y,exog = X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-square,RMSE,MAE,MAPE (TRAINING DATA) After Selecting Features(From Boruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting the test set results\n",
    "train_pred =regressor.predict(X_train)\n",
    "\n",
    "##  MAPE\n",
    "import numpy as np\n",
    "def mean_absolute_percentage_error(Y_train,train_pred): \n",
    "    Y_train, train_pred = np.array(Y_train), np.array(train_pred)\n",
    "    return np.mean(np.abs((Y_train,train_pred) / Y_train)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Training Data = 70.2461417113\n",
      "RMSE of Training Data = 112.96096481830213\n",
      "R2 of Training Data = -0.163192594669\n",
      "MAPE of Training Data = 129.204229216\n"
     ]
    }
   ],
   "source": [
    "## R-squared score of this model\n",
    "from sklearn.metrics import*\n",
    "import math\n",
    "## Mean absolute error\n",
    "MAE = mean_absolute_error(Y_train,train_pred)\n",
    "print('MAE of Training Data =', MAE)\n",
    "## Mean squared error\n",
    "MSE = mean_squared_error(Y_train,train_pred)\n",
    "RMSE = math.sqrt(MSE)\n",
    "print('RMSE of Training Data =',RMSE)\n",
    "## R-square score of this model\n",
    "R2 = r2_score(Y_train,train_pred)\n",
    "print('R2 of Training Data =',R2)\n",
    "## MAPE of this model\n",
    "MAPE=mean_absolute_percentage_error(Y_train,train_pred)\n",
    "print('MAPE of Training Data =',MAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-square,RMSE,MAE,MAPE (TESTING DATA) After Selecting Features (From boruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Predicting the test set results\n",
    "test_pred =regressor.predict(X_test)\n",
    "\n",
    "## MAPE\n",
    "import numpy as np\n",
    "def mean_absolute_percentage_error(Y_test,test_pred): \n",
    "    Y_test, test_pred = np.array(Y_test), np.array(test_pred)\n",
    "    return np.mean(np.abs((Y_test,test_pred) / Y_test)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Testing Data= 56.2796865285\n",
      "RMSE of Testing Data= 96.2497906300326\n",
      "R2 of Testing Data= 0.125543557022\n",
      "MAPE of Testing Data= 122.967874807\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "##Mean absolute error\n",
    "MAE = mean_absolute_error(Y_test,test_pred)\n",
    "print('MAE of Testing Data=',MAE)\n",
    "## Mean squared error\n",
    "MSE = mean_squared_error(Y_test,test_pred)\n",
    "RMSE = math.sqrt(MSE)\n",
    "print('RMSE of Testing Data=',RMSE)\n",
    "## R-square score of this model\n",
    "R2 = r2_score(Y_test,test_pred)\n",
    "print('R2 of Testing Data=',R2)\n",
    "## MAPE of this model\n",
    "MAPE=mean_absolute_percentage_error(Y_test,test_pred)\n",
    "print('MAPE of Testing Data=',MAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FORWARD SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1= dataset[[\n",
    "       'Kitchen_Temp', 'Kitchen_Hum', 'LivingRoom_Temp', 'LivingRoom_Hum',\n",
    "       'LaundryRoom_Temp', 'LaundryRoom_Hum', 'OfficeRoom_Temp',\n",
    "       'OfficeRoom_Hum', 'BathRoom_Temp', 'BathRoom_Hum', 'OutsideNorth_Temp',\n",
    "       'OutsideNorth_Hum', 'IroningRoom_Temp', 'IroningRoom_Hum',\n",
    "       'TeenagerRoom_Temp', 'TeenagerRoom_Hum', 'ParentRoom_Temp',\n",
    "       'ParentRoom_Hum', 'Outside_Temp', 'Pressure', 'Humidity', 'Windspeed',\n",
    "       'Visibility', 'Tdewpoint', 'rv1', 'Month_Number', 'Weekday_number']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y1=dataset['Energy_consumed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add  LaundryRoom_Temp               with p-value 5.4523e-129\n",
      "resulting features:\n",
      "['LaundryRoom_Temp']\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "def stepwise_selection(X1, y1, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X1.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y1, sm.add_constant(X1)).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.argmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "            if not changed:\n",
    "                break\n",
    "            return included\n",
    "\n",
    "result = stepwise_selection(X1, y1)\n",
    "\n",
    "print('resulting features:')\n",
    "print(result)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using Forward selection method we only get one variable as an important feature. So wwe will try backward Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACKWARD ELIMINATION METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resulting features:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "def stepwise_selection(X1, y1, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "        included = list(initial_list)\n",
    "        while True:\n",
    "            changed=False\n",
    "import statsmodels.api as sm\n",
    "def stepwise_selection(X1, y1, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "         # backward step\n",
    "        model = sm.OLS(y1, sm.add_constant(pd.DataFrame(X1[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included\n",
    "\n",
    "result = stepwise_selection(X1, y1)\n",
    "\n",
    "print('resulting features:')\n",
    "print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using Backward Elimination method we do not get any variable as an important feature.We will now try Exhaustive Search Method which is a combination of both forward selection and backward Elmination method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXHAUSTIVE SEARCH METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add  Humidity                       with p-value 2.22825e-92\n",
      "Add  Kitchen_Hum                    with p-value 1.2589e-85\n",
      "Add  TeenagerRoom_Hum               with p-value 4.48028e-143\n",
      "Add  LivingRoom_Hum                 with p-value 1.8353e-101\n",
      "Add  Month_Number                   with p-value 1.90285e-35\n",
      "Drop Humidity                       with p-value 0.616498\n",
      "Add  LaundryRoom_Temp               with p-value 6.44506e-75\n",
      "Add  ParentRoom_Temp                with p-value 4.20613e-71\n",
      "Add  LivingRoom_Temp                with p-value 7.6848e-31\n",
      "Add  TeenagerRoom_Temp              with p-value 1.32161e-32\n",
      "Add  OutsideNorth_Temp              with p-value 7.97415e-23\n",
      "Add  Outside_Temp                   with p-value 5.87504e-15\n",
      "Add  Windspeed                      with p-value 1.05919e-06\n",
      "Add  OfficeRoom_Temp                with p-value 9.88067e-08\n",
      "Add  Tdewpoint                      with p-value 7.5834e-08\n",
      "Add  LaundryRoom_Hum                with p-value 1.27303e-08\n",
      "Add  ParentRoom_Hum                 with p-value 8.99433e-09\n",
      "Add  IroningRoom_Hum                with p-value 2.88288e-06\n",
      "Add  OfficeRoom_Hum                 with p-value 7.25068e-05\n",
      "Add  BathRoom_Hum                   with p-value 0.00187506\n",
      "Add  Visibility                     with p-value 0.00280772\n",
      "Add  Weekday_number                 with p-value 0.00297726\n",
      "resulting features:\n",
      "['Kitchen_Hum', 'TeenagerRoom_Hum', 'LivingRoom_Hum', 'Month_Number', 'LaundryRoom_Temp', 'ParentRoom_Temp', 'LivingRoom_Temp', 'TeenagerRoom_Temp', 'OutsideNorth_Temp', 'Outside_Temp', 'Windspeed', 'OfficeRoom_Temp', 'Tdewpoint', 'LaundryRoom_Hum', 'ParentRoom_Hum', 'IroningRoom_Hum', 'OfficeRoom_Hum', 'BathRoom_Hum', 'Visibility', 'Weekday_number']\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "def stepwise_selection(X1, y1, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X1.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y1, sm.add_constant(pd.DataFrame(X1[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.argmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y1, sm.add_constant(pd.DataFrame(X1[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included\n",
    "\n",
    "result = stepwise_selection(X1, y1)\n",
    "\n",
    "print('resulting features:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SELECTION AFTER EXHAUSTIVE SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 90,  90,  80, ..., 280, 430, 440], dtype=int64)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## X are the varibles used for predictions\n",
    "## Y is the target\n",
    "X = dataset.iloc[:,[7,21,9,31,10,22,8,20,16,24,27,12,29,11,23,19,13,15,28,32]].values\n",
    "Y = dataset.iloc[:,3].values\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## fitting multiple linear regression to the training set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=np.random)\n",
    "regressor.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th>  <td>   0.156</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.156</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   182.9</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 16 Mar 2018</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:30:49</td>     <th>  Log-Likelihood:    </th> <td>-1.1805e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 19735</td>      <th>  AIC:               </th>  <td>2.361e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 19714</td>      <th>  BIC:               </th>  <td>2.363e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    20</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   68.9145</td> <td>   21.247</td> <td>    3.243</td> <td> 0.001</td> <td>   27.268</td> <td>  110.561</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   16.5993</td> <td>    0.593</td> <td>   27.987</td> <td> 0.000</td> <td>   15.437</td> <td>   17.762</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -6.3374</td> <td>    0.352</td> <td>  -17.985</td> <td> 0.000</td> <td>   -7.028</td> <td>   -5.647</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>  -14.7608</td> <td>    0.701</td> <td>  -21.043</td> <td> 0.000</td> <td>  -16.136</td> <td>  -13.386</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>  -13.7872</td> <td>    1.575</td> <td>   -8.752</td> <td> 0.000</td> <td>  -16.875</td> <td>  -10.699</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   26.1841</td> <td>    0.971</td> <td>   26.962</td> <td> 0.000</td> <td>   24.281</td> <td>   28.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>  -21.1869</td> <td>    1.410</td> <td>  -15.023</td> <td> 0.000</td> <td>  -23.951</td> <td>  -18.423</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>  -21.7919</td> <td>    1.270</td> <td>  -17.160</td> <td> 0.000</td> <td>  -24.281</td> <td>  -19.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>   10.7774</td> <td>    0.840</td> <td>   12.825</td> <td> 0.000</td> <td>    9.130</td> <td>   12.425</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    6.9340</td> <td>    0.578</td> <td>   12.000</td> <td> 0.000</td> <td>    5.801</td> <td>    8.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   -7.3178</td> <td>    0.657</td> <td>  -11.145</td> <td> 0.000</td> <td>   -8.605</td> <td>   -6.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>    1.9457</td> <td>    0.333</td> <td>    5.845</td> <td> 0.000</td> <td>    1.293</td> <td>    2.598</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>    6.8767</td> <td>    0.973</td> <td>    7.067</td> <td> 0.000</td> <td>    4.969</td> <td>    8.784</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>    3.2450</td> <td>    0.533</td> <td>    6.086</td> <td> 0.000</td> <td>    2.200</td> <td>    4.290</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>    4.6295</td> <td>    0.690</td> <td>    6.708</td> <td> 0.000</td> <td>    3.277</td> <td>    5.982</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>   -2.0383</td> <td>    0.403</td> <td>   -5.064</td> <td> 0.000</td> <td>   -2.827</td> <td>   -1.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>   -2.1319</td> <td>    0.397</td> <td>   -5.371</td> <td> 0.000</td> <td>   -2.910</td> <td>   -1.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>    2.6009</td> <td>    0.636</td> <td>    4.088</td> <td> 0.000</td> <td>    1.354</td> <td>    3.848</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>    0.2729</td> <td>    0.086</td> <td>    3.184</td> <td> 0.001</td> <td>    0.105</td> <td>    0.441</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    0.1886</td> <td>    0.059</td> <td>    3.197</td> <td> 0.001</td> <td>    0.073</td> <td>    0.304</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>    1.0697</td> <td>    0.360</td> <td>    2.970</td> <td> 0.003</td> <td>    0.364</td> <td>    1.776</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13805.349</td> <th>  Durbin-Watson:     </th>  <td>   0.601</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>202718.842</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 3.279</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>17.266</td>   <th>  Cond. No.          </th>  <td>4.14e+03</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.156\n",
       "Model:                            OLS   Adj. R-squared:                  0.156\n",
       "Method:                 Least Squares   F-statistic:                     182.9\n",
       "Date:                Fri, 16 Mar 2018   Prob (F-statistic):               0.00\n",
       "Time:                        17:30:49   Log-Likelihood:            -1.1805e+05\n",
       "No. Observations:               19735   AIC:                         2.361e+05\n",
       "Df Residuals:                   19714   BIC:                         2.363e+05\n",
       "Df Model:                          20                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         68.9145     21.247      3.243      0.001      27.268     110.561\n",
       "x1            16.5993      0.593     27.987      0.000      15.437      17.762\n",
       "x2            -6.3374      0.352    -17.985      0.000      -7.028      -5.647\n",
       "x3           -14.7608      0.701    -21.043      0.000     -16.136     -13.386\n",
       "x4           -13.7872      1.575     -8.752      0.000     -16.875     -10.699\n",
       "x5            26.1841      0.971     26.962      0.000      24.281      28.088\n",
       "x6           -21.1869      1.410    -15.023      0.000     -23.951     -18.423\n",
       "x7           -21.7919      1.270    -17.160      0.000     -24.281     -19.303\n",
       "x8            10.7774      0.840     12.825      0.000       9.130      12.425\n",
       "x9             6.9340      0.578     12.000      0.000       5.801       8.067\n",
       "x10           -7.3178      0.657    -11.145      0.000      -8.605      -6.031\n",
       "x11            1.9457      0.333      5.845      0.000       1.293       2.598\n",
       "x12            6.8767      0.973      7.067      0.000       4.969       8.784\n",
       "x13            3.2450      0.533      6.086      0.000       2.200       4.290\n",
       "x14            4.6295      0.690      6.708      0.000       3.277       5.982\n",
       "x15           -2.0383      0.403     -5.064      0.000      -2.827      -1.249\n",
       "x16           -2.1319      0.397     -5.371      0.000      -2.910      -1.354\n",
       "x17            2.6009      0.636      4.088      0.000       1.354       3.848\n",
       "x18            0.2729      0.086      3.184      0.001       0.105       0.441\n",
       "x19            0.1886      0.059      3.197      0.001       0.073       0.304\n",
       "x20            1.0697      0.360      2.970      0.003       0.364       1.776\n",
       "==============================================================================\n",
       "Omnibus:                    13805.349   Durbin-Watson:                   0.601\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           202718.842\n",
       "Skew:                           3.279   Prob(JB):                         0.00\n",
       "Kurtosis:                      17.266   Cond. No.                     4.14e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.14e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "X = np.append(arr=np.ones((19735,1)).astype(int),values = X,axis = 1)\n",
    "X_opt = X[:,0:32]\n",
    "regressor_OLS = sm.OLS(endog = Y,exog = X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-square,RMSE,MAE,MAPE (TRAINING DATA) After Selecting Features from Exhaustive search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting the test set results\n",
    "train_pred =regressor.predict(X_train)\n",
    "\n",
    "##  MAPE\n",
    "import numpy as np\n",
    "def mean_absolute_percentage_error(Y_train,train_pred): \n",
    "    Y_train, train_pred = np.array(Y_train), np.array(train_pred)\n",
    "    return np.mean(np.abs((Y_train,train_pred) / Y_train)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Training Data = 54.3393952879\n",
      "RMSE of Training Data = 95.32743306844182\n",
      "R2 of Training Data = 0.158198751503\n",
      "MAPE of Training Data = 121.118234923\n"
     ]
    }
   ],
   "source": [
    "## R-squared score of this model\n",
    "from sklearn.metrics import*\n",
    "import math\n",
    "## Mean absolute error\n",
    "MAE = mean_absolute_error(Y_train,train_pred)\n",
    "print('MAE of Training Data =', MAE)\n",
    "## Mean squared error\n",
    "MSE = mean_squared_error(Y_train,train_pred)\n",
    "RMSE = math.sqrt(MSE)\n",
    "print('RMSE of Training Data =',RMSE)\n",
    "## R-square score of this model\n",
    "R2 = r2_score(Y_train,train_pred)\n",
    "print('R2 of Training Data =',R2)\n",
    "## MAPE of this model\n",
    "MAPE=mean_absolute_percentage_error(Y_train,train_pred)\n",
    "print('MAPE of Training Data =',MAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-square,RMSE,MAE,MAPE (TESTING DATA) After Selecting Features from Exhaustive search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Predicting the test set results\n",
    "test_pred =regressor.predict(X_test)\n",
    "\n",
    "## MAPE\n",
    "import numpy as np\n",
    "def mean_absolute_percentage_error(Y_test,test_pred): \n",
    "    Y_test, test_pred = np.array(Y_test), np.array(test_pred)\n",
    "    return np.mean(np.abs((Y_test,test_pred) / Y_test)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Testing Data= 55.4967939596\n",
      "RMSE of Testing Data= 98.04630816693702\n",
      "R2 of Testing Data= 0.148423785338\n",
      "MAPE of Testing Data= 119.996039667\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "##Mean absolute error\n",
    "MAE = mean_absolute_error(Y_test,test_pred)\n",
    "print('MAE of Testing Data=',MAE)\n",
    "## Mean squared error\n",
    "MSE = mean_squared_error(Y_test,test_pred)\n",
    "RMSE = math.sqrt(MSE)\n",
    "print('RMSE of Testing Data=',RMSE)\n",
    "## R-square score of this model\n",
    "R2 = r2_score(Y_test,test_pred)\n",
    "print('R2 of Testing Data=',R2)\n",
    "## MAPE of this model\n",
    "MAPE=mean_absolute_percentage_error(Y_test,test_pred)\n",
    "print('MAPE of Testing Data=',MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
